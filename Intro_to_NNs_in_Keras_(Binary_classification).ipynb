{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyx462/ML/blob/main/Intro_to_NNs_in_Keras_(Binary_classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v3rhgaG7ABJR"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_Uu0LjABJR"
      },
      "source": [
        "# Introduction to Deep Learning with Keras and TensorFlow\n",
        "**MATH498: Foundations of ML (University of Michigan)**\n",
        "\n",
        "**Modified notebook from Daniel Moser (UT Southwestern Medical Center), to use keras v.2.8.0 and some further exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM7uiaPGABJV"
      },
      "source": [
        "## Prerequisite Python Modules\n",
        "\n",
        "First, some software needs to be loaded into the Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1wQbcawABJV"
      },
      "outputs": [],
      "source": [
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential  # Model type to be used\n",
        "from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n",
        "from keras.utils import np_utils                         # NumPy related tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Mz8BFiABJW"
      },
      "source": [
        "## Loading Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mgDItutABJX"
      },
      "outputs": [],
      "source": [
        "# The Wisconsin Breast Cancer data dataset (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e3fEaXE7ABJb"
      },
      "outputs": [],
      "source": [
        "nb_classes = 2 # number of unique digits\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xw2pA3a1ABJc"
      },
      "outputs": [],
      "source": [
        "# The Sequential model is a linear stack of layers and is very common.\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNbeoXVGABJc"
      },
      "source": [
        "## The first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V12PfxtfABJc"
      },
      "outputs": [],
      "source": [
        "# The first hidden layer is a set of 64 nodes (artificial neurons).\n",
        "# Each node will receive an element from each input vector and apply some weight and bias to it.\n",
        "\n",
        "# An \"activation\" is a non-linear function applied to the output of the layer above.\n",
        "# It checks the new value of the node, and decides whether that artifical neuron has fired.\n",
        "# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n",
        "# Those inputs are then not considered to be fired.\n",
        "# Positive values of a node are unchanged.\n",
        "\n",
        "model.add(Dense(64, input_shape=(30,), activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "89arkGYzABJd"
      },
      "outputs": [],
      "source": [
        "# Dropout zeroes a selection of random outputs (i.e., disables their activation)\n",
        "# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
        "model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE0uOvv6ABJe"
      },
      "source": [
        "## Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UoBQnhPIABJe"
      },
      "outputs": [],
      "source": [
        "# The second hidden layer appears identical to our first layer.\n",
        "# However, instead of each of the 64-node receiving 30-inputs from the input image data,\n",
        "# they receive 64 inputs from the output of the first 64-node layer.\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqRejG2tABJe"
      },
      "source": [
        "## The Final Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aM7MfO8YABJe"
      },
      "outputs": [],
      "source": [
        "# The final layer of 2 neurons in fully-connected to the previous 64-node layer.\n",
        "# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n",
        "\n",
        "# The \"softmax\" activation represents a probability distribution over K different possible outcomes.\n",
        "# Its values are all non-negative and sum to 1.\n",
        "model.add(Dense(nb_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-6ygHS-ABJf"
      },
      "outputs": [],
      "source": [
        "# Summarize the built model\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATWE1bvSABJf"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "Our predictions are probability distributions across the two possible outcomes (e.g. \"we're 80% confident this is 1, 20% sure it's a 0\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p7upMPxAABJh"
      },
      "outputs": [],
      "source": [
        "# Let's use the Adam optimizer for learning\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNBXytWeABJh"
      },
      "source": [
        "## Train the model!\n",
        "This is the fun part! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN0Y3EbsABJh"
      },
      "source": [
        "The batch size determines over how much data per step is used to compute the loss function, gradients, and back propagation. Large batch sizes allow the network to complete it's training faster; however, there are other factors beyond training speed to consider.\n",
        "\n",
        "Too large of a batch size smoothes the local minima of the loss function, causing the optimizer to settle in one because it thinks it found the global minimum.\n",
        "\n",
        "Too small of a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.\n",
        "\n",
        "So a good batch size may take some trial and error to find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khmKI01DABJh"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=20, validation_split=0.1,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gefSqmLFOR4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB8bXbwkABJh"
      },
      "source": [
        "The two numbers, in order, represent the value of the loss function of the network on the training set, and the overall accuracy of the network on the training data. But how does it do on data it did not train on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZED7xDwABJh"
      },
      "source": [
        "## Evaluate Model's Accuracy on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyjHcYg_ABJh"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, Y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with different settings:\n",
        "- number of epochs\n",
        "- different architectures (i.e. number of hidden layers, activation function, width of layers)\n",
        "- different initialisation\n",
        "- different optimisers"
      ],
      "metadata": {
        "id": "305Rp2zPLSBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Sequential model is a linear stack of layers and is very common.\n",
        "# Define the architecture\n",
        "\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "new_model = Sequential()\n",
        "new_model.add(Dense(64, input_shape=(30,), activation='relu',kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros'),) # input layer must have the right input shape\n",
        "new_model.add(Dropout(0.2))\n",
        "\n",
        "new_model.add(Dense(64, activation='relu',kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros'))\n",
        "new_model.add(Dropout(0.2))\n",
        "\n",
        "new_model.add(Dense(nb_classes, activation='softmax',kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros')) # output layer must have the right output shape\n",
        "\n",
        "# Choose how to optimize the model\n",
        "new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# this is also where stopping_criteria can be added\n",
        "\n",
        "# Choose how to train the model\n",
        "new_model.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=20, validation_split=0.1,\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2F78ckHLlOO",
        "outputId": "1011851e-6845-4cb1-af3c-6243a05ba878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "12/12 [==============================] - 2s 43ms/step - loss: 41.4503 - accuracy: 0.5718 - val_loss: 8.2386 - val_accuracy: 0.3953\n",
            "Epoch 2/20\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 32.0072 - accuracy: 0.5535 - val_loss: 13.2966 - val_accuracy: 0.3721\n",
            "Epoch 3/20\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 24.1923 - accuracy: 0.6084 - val_loss: 4.9567 - val_accuracy: 0.5581\n",
            "Epoch 4/20\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 16.8681 - accuracy: 0.6789 - val_loss: 2.8082 - val_accuracy: 0.7442\n",
            "Epoch 5/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 14.3120 - accuracy: 0.5875 - val_loss: 1.1754 - val_accuracy: 0.9070\n",
            "Epoch 6/20\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 9.3377 - accuracy: 0.7128 - val_loss: 1.3563 - val_accuracy: 0.8837\n",
            "Epoch 7/20\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.9183 - accuracy: 0.7050 - val_loss: 1.4422 - val_accuracy: 0.8372\n",
            "Epoch 8/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.6936 - accuracy: 0.7050 - val_loss: 3.3087 - val_accuracy: 0.6977\n",
            "Epoch 9/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.0953 - accuracy: 0.6893 - val_loss: 1.3188 - val_accuracy: 0.8372\n",
            "Epoch 10/20\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.9265 - accuracy: 0.7467 - val_loss: 1.8063 - val_accuracy: 0.8140\n",
            "Epoch 11/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4.9632 - accuracy: 0.7520 - val_loss: 1.4505 - val_accuracy: 0.8372\n",
            "Epoch 12/20\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 4.9924 - accuracy: 0.7415 - val_loss: 1.8871 - val_accuracy: 0.7907\n",
            "Epoch 13/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.0286 - accuracy: 0.7128 - val_loss: 0.6660 - val_accuracy: 0.8837\n",
            "Epoch 14/20\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.0818 - accuracy: 0.7232 - val_loss: 0.6495 - val_accuracy: 0.9070\n",
            "Epoch 15/20\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0534 - accuracy: 0.7650 - val_loss: 0.6161 - val_accuracy: 0.8837\n",
            "Epoch 16/20\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3.8582 - accuracy: 0.7572 - val_loss: 0.7104 - val_accuracy: 0.8605\n",
            "Epoch 17/20\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.9886 - accuracy: 0.7520 - val_loss: 0.6119 - val_accuracy: 0.8837\n",
            "Epoch 18/20\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.4174 - accuracy: 0.7911 - val_loss: 0.6820 - val_accuracy: 0.8837\n",
            "Epoch 19/20\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.3850 - accuracy: 0.7859 - val_loss: 0.6150 - val_accuracy: 0.8837\n",
            "Epoch 20/20\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.0452 - accuracy: 0.7937 - val_loss: 0.6760 - val_accuracy: 0.8837\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ec666f6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Tensorflow (GPU)",
      "language": "python",
      "name": "py3.6-tfgpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Intro to NNs in Keras (Binary classification).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}